{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# the cool_fusion function are in source/cool_fusion.``py\n",
    "from source.cool_fusion import CoolFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"your key here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the model names\n",
    "# model_name1 = \"Azurro/APT3-1B-Instruct-v1\"\n",
    "# model_name2 = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "model_name3 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name4 = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "\n",
    "# Load tokenizers\n",
    "# tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "# tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name3)\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(model_name4)\n",
    "\n",
    "# Load models \n",
    "# model1 = AutoModelForCausalLM.from_pretrained(model_name1).to(device)\n",
    "# model2 = AutoModelForCausalLM.from_pretrained(model_name2).to(device)\n",
    "model3 = AutoModelForCausalLM.from_pretrained(model_name3).to(device)\n",
    "model4 = AutoModelForCausalLM.from_pretrained(model_name4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/users/eleves-a/2021/darius.dabert/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled in the heart of a dense forest, there lived a young girl named Lily. Lily was a curious and adventurous child, with a love for nature and all its wonders. She spent most of her days exploring the forest, collecting wildflowers, and watching the animals that lived there.\n",
      "\n",
      "One day, while wandering through the forest, Lily stumbled upon a hidden path that she had never seen before. The path was narrow and winding, and it seemed to lead to a place she had never been before. Lily felt a thrill of excitement as she\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "fused_model = CoolFusion(\n",
    "    models={ \"llama\": model3, \"qwen\": model4},\n",
    "    tokenizers={\"llama\": tokenizer3,  \"qwen\": tokenizer4},\n",
    ")\n",
    "\n",
    "context = 'Once upon a time'\n",
    "generated_text, generate_id, seg_ppl = fused_model.generate(context, max_length=len(context.split()) + 99, verbose=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
