{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# the cool_fusion function are in source/cool_fusion.py\n",
    "from source.cool_fusion import CoolFusion\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# Define the model names\n",
    "model_name1 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name2 = \"Azurro/APT3-1B-Instruct-v1\"\n",
    "# Load tokenizers\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "\n",
    "# Load models (using CPU by default; add device_map=\"auto\" for GPU)\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_name1).to(device)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(model_name2,trust_remote_code=True).to(device)\n",
    "\n",
    "# Sample prompt for generation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "\n",
    "fused_model = CoolFusion(\n",
    "    models={\"1\": model1, \"2\": model2},\n",
    "    tokenizers={\"1\": tokenizer1, \"2\": tokenizer2}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_prompt(row):\n",
    "    prompt = f\"Question: {row[0]}\\nChoices:\\n\"\n",
    "    for i,option in enumerate(['A', 'B', 'C', 'D']):\n",
    "        prompt += f\"{option}. {row[i+1]}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "def format_answer(row):\n",
    "    return row[-2]\n",
    "def extract_answer(output_text):\n",
    "    # Very basic extraction â€” adjust for your model's output style\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice in output_text:\n",
    "            return choice  \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data/test/anatomy_test.csv\")\n",
    "df[\"prompt\"] = df.apply(format_prompt,axis=1)\n",
    "df[\"answer\"] = df.apply(format_answer, axis=1)\n",
    "\n",
    "row = df.iloc[2]\n",
    "print(row.iloc)\n",
    "input = row[\"prompt\"]\n",
    "print(len(input))\n",
    "answer = row[\"answer\"]\n",
    "print(input)\n",
    "print(answer)\n",
    "output = fused_model.generate(input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subject(subject_path,fused_model):\n",
    "    df = pd.read_csv(subject_path)\n",
    "    df[\"prompt\"] = df.apply(format_prompt, axis=1)\n",
    "    df[\"answer\"] = df.apply(format_answer, axis=1)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=os.path.basename(subject_path)):\n",
    "        input = row['prompt']\n",
    "        \n",
    "        output = fused_model.generate(input,max_length=len(input.split()) + 5)\n",
    "        \n",
    "        pred = extract_answer(output[len(row['prompt']):])  # trim prompt from generated\n",
    "        \n",
    "        if pred == row['answer']:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_subject(\"data/test/high_school_european_history_test.csv\",fused_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
